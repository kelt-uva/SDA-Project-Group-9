{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.metrics import balanced_accuracy_score, brier_score_loss, accuracy_score, roc_auc_score, roc_curve, auc\n",
    "from scipy.stats import multivariate_normal, bernoulli, beta, norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data/CVD_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the entire data, with one singular plot per column in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_columns(data: pd.DataFrame) -> None:\n",
    "    \"\"\"Plot the columns of a dataframe, where every\n",
    "    column is plotted as a single histogram or bar plot.\n",
    "    A column with only 0 or 1 as its values is plotted\n",
    "    as a bar plot and will get ticks 'Yes' or 'No'.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): any pandas dataframe.\n",
    "    \"\"\"\n",
    "    plots_per_row = 3\n",
    "    num_cols = len(data.columns)\n",
    "    num_rows = np.ceil(num_cols / plots_per_row).astype(int)\n",
    "\n",
    "    # Figsize is hand picked\n",
    "    fig = plt.figure(figsize=(15, 8 * num_rows))\n",
    "\n",
    "    for i, col in enumerate(data.columns):\n",
    "        ax = fig.add_subplot(num_rows, plots_per_row, i + 1)\n",
    "\n",
    "        # If everything is numbers, we make a histogram\n",
    "        if all([isinstance(x, (int, float)) for x in data[col]]):\n",
    "            ax.hist(data[col], bins=30, edgecolor='k', color='c')\n",
    "            m = np.mean(data[col])\n",
    "            s = np.std(data[col])\n",
    "            ax.axvline(m, color='red', linestyle='--', label=fr'Mean $\\mu$')\n",
    "            ax.axvline(m + s, color='green', linestyle='--', label=fr'Mean $\\mu \\mp $ std $\\sigma$')\n",
    "            ax.axvline(m - s, color='green', linestyle='--')\n",
    "            ax.legend()\n",
    "            \n",
    "        else:\n",
    "        # Otherwise it must be a bar plot\n",
    "            ax.bar(data[col].unique(), data[col].value_counts(), edgecolor='k', color='c')\n",
    "            # Check if only 0 and 1, then we must change the ticks\n",
    "            if all([x in [0, 1] for x in data[col]]):\n",
    "                ax.set_xticks([0, 1])\n",
    "                ax.set_xticklabels(['No', 'Yes'])\n",
    "        \n",
    "        ax.tick_params(axis='x', rotation=90)\n",
    "        ax.set_title(col)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_data_columns(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now visualise the data for only females:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_columns(data.loc[data[\"Sex\"] == \"Female\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now visualise the data for only males:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_columns(data.loc[data[\"Sex\"] == \"Male\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The correlation matrix for our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the correlation we can only take columns with numbers\n",
    "data_num = data.select_dtypes(include=[int, float])\n",
    "corr = data_num.corr()\n",
    "\n",
    "plt.imshow(corr, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(corr.columns)), corr.columns, rotation=90)\n",
    "plt.yticks(np.arange(len(corr.columns)), corr.columns)\n",
    "plt.title('Correlation matrix for the dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformatting the data for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformatting the data to be suitable for logistic regression\n",
    "# transforming categorical variables (Yes=1, No=0)\n",
    "# transforming 'Sex' variable to binary 'Sex_Male' variable\n",
    "data = data.replace({'Skin_Cancer':'Yes', 'Other_Cancer': 'Yes', 'Heart_Disease': 'Yes', 'Depression': 'Yes', 'Smoking_History': 'Yes', 'Exercise': 'Yes'\n",
    "                     , 'Sex': 'Male', 'Arthritis': 'Yes'}, 1).replace({'Skin_Cancer':'No', 'Other_Cancer': 'No', 'Heart_Disease': 'No'\n",
    "                                                   , 'Depression': 'No', 'Smoking_History': 'No', 'Exercise': 'No'\n",
    "                                                   , 'Sex': 'Female', 'Arthritis': 'No'}, 0).rename(columns={'Sex':'Sex_Male'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking columns with multiple categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['Age_Category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['Diabetes'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['Checkup'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['General_Health'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation of numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize numerical values\n",
    "def normalize_data(data):\n",
    "    \"\"\"\n",
    "    This function takes a column of data and normalizes it.\n",
    "    \"\"\"\n",
    "    min_value = min(data)\n",
    "    max_value = max(data)\n",
    "    normalized_data = []\n",
    "\n",
    "    for value in data:\n",
    "        new_value = (value - min_value) / (max_value - min_value)\n",
    "        normalized_data.append(new_value)\n",
    "\n",
    "    return normalized_data\n",
    "\n",
    "# normalise the height (done in such a way that we can interpret an increase in height as a 10 cm increase over the average height)\n",
    "data['Height_(cm)'] = (data['Height_(cm)']- data['Height_(cm)'].mean())/10\n",
    "data = data.rename(columns={'Height_(cm)':'Height_norm'})\n",
    "\n",
    "# normalise the weight (done in such a way that we can interpret an increase in weight as a 5 kg increase over the average weight)\n",
    "data['Weight_(kg)'] = (data['Weight_(kg)']- data['Weight_(kg)'].mean())/5\n",
    "data = data.rename(columns={'Weight_(kg)':'Weight_norm'})\n",
    "\n",
    "# normalise the bmi (done in such a way that we can interpret an increase in bmi as a 1 point increase over the average bmi)\n",
    "data['BMI'] = (data['BMI']- data['BMI'].mean())\n",
    "data = data.rename(columns={'BMI':'BMI_norm'})\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the remaining columns (multiple categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set age to numerical by taking the mean of the existing categories (ASSUMPTION: 80+ is estimated to 85)\n",
    "age_mapping = {'18-24': 21.0, '25-29': 27.0, '30-34': 32.0, '35-39': 37.0,'40-44': 42.0, '45-49': 47.0, '50-54': 52.0, \n",
    "               '55-59': 57.0,'60-64': 62.0, '65-69': 67.0, '70-74': 72.0, '75-79': 77.0, '80+': 85.0}\n",
    "\n",
    "# apply the mapping\n",
    "data['Age_Category'] = data['Age_Category'].map(age_mapping)\n",
    "data = data.rename(columns={'Age_Category':'Age_Numeric'})\n",
    "\n",
    "# normalise the age (done in such a way that we can interpret an increase in age as a 10 year increase over the average age)\n",
    "data['Age_Numeric'] = (data['Age_Numeric']- data['Age_Numeric'].mean())/10\n",
    "data = data.rename(columns={'Age_Numeric':'Age_Normalised'})\n",
    "\n",
    "# diabetes transformation\n",
    "# group all kinds of diabetics together\n",
    "diabetes_binary = [0 if i == 'No' else 1 for i in data['Diabetes']]\n",
    "data['Diabetes'] = diabetes_binary\n",
    "data = data.rename(columns={'Diabetes':'Diabetic'})\n",
    "\n",
    "# group 'recent' checkups (within past 2 years)\n",
    "checkup_binary = [1 if i == 'Within the past year' or i == 'Within the past 2 years' else 0 for i in data['Checkup']]\n",
    "data['Checkup'] = checkup_binary\n",
    "data = data.rename(columns={'Checkup':'Recent_Checkup'})\n",
    "\n",
    "# transform perceived health\n",
    "health_binary = [0 if i == 'Fair' or i == 'Poor' else 1 for i in data['General_Health']]\n",
    "data['General_Health'] = health_binary\n",
    "data = data.rename(columns={'General_Health':'Health_Good_or_Better'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping consumption columns (without knowing exactly what their contents mean they are useless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['Alcohol_Consumption', 'Fruit_Consumption', 'Green_Vegetables_Consumption', 'FriedPotato_Consumption'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# calculate/plot correlation coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots for the bars/histograms based on if a person has heart disease or not, to possibly see correlations between a variable and heart disease (or not). It shows the difference between the group with and without heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_heart_disease = data.loc[data[\"Heart_Disease\"] == 0]\n",
    "heart_disease = data.loc[data[\"Heart_Disease\"] == 1]\n",
    "\n",
    "no_heart_disease_y = no_heart_disease[\"Heart_Disease\"] # y vector\n",
    "heart_disease_y = heart_disease[\"Heart_Disease\"] # y vector\n",
    "\n",
    "no_heart_disease = no_heart_disease.drop(columns=[\"Heart_Disease\"])\n",
    "heart_disease = heart_disease.drop(columns=[\"Heart_Disease\"])\n",
    "\n",
    "fig, axs = plt.subplots(5, 3, figsize=(25, 30))\n",
    "\n",
    "c = 0\n",
    "for i in range(14):\n",
    "\n",
    "    plt.subplot(5, 3, i + 1)\n",
    "    plt.hist(no_heart_disease[no_heart_disease.columns[i]], label='No heart disease', alpha=0.5, color='b')\n",
    "    plt.hist(heart_disease[heart_disease.columns[i]], label='Heart disease', alpha=0.5, color='orange')\n",
    "    plt.xlabel(no_heart_disease.columns[i])\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Data of {no_heart_disease.columns[i]} for people with and without heart disease')\n",
    "\n",
    "    unique_vals = no_heart_disease[no_heart_disease.columns[i]].unique()\n",
    "    if len(unique_vals) == 2 and all([x in [0, 1] for x in unique_vals]):\n",
    "        plt.xticks([0, 1], ['No', 'Yes'])\n",
    "\n",
    "    plt.legend()\n",
    "    c += 1\n",
    "\n",
    "# remove empty subplot\n",
    "fig.delaxes(axs[-1, -1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a basic logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into X and Y and adding a constant\n",
    "Y = data['Heart_Disease']\n",
    "X = data.drop(columns='Heart_Disease')\n",
    "X_intercept = sm.add_constant(X)\n",
    "\n",
    "# recombining XY\n",
    "column_names = ['constant']\n",
    "for i in data.columns:\n",
    "    column_names.append(i)\n",
    "\n",
    "XY_constant = pd.DataFrame(np.hstack((np.array(Y)[:, np.newaxis], X_intercept)), columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into test and train set (25:75)\n",
    "num_samples = 0.25 * len(XY_constant)\n",
    "row_ids = list(range(XY_constant.shape[0]))\n",
    "\n",
    "# randomly select 25% of the row ids\n",
    "selected_row_ids = random.sample(row_ids, round(num_samples))\n",
    "\n",
    "# subset to create train and test\n",
    "data_test = XY_constant.iloc[XY_constant.index.isin(selected_row_ids)]\n",
    "data_train = XY_constant.iloc[~XY_constant.index.isin(selected_row_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = data_train['Heart_Disease']\n",
    "Y_test = data_test['Heart_Disease']\n",
    "X_train = data_train.drop(columns='Heart_Disease')\n",
    "X_test = data_test.drop(columns='Heart_Disease')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.Logit(Y_train, X_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions\n",
    "predictions = model.predict(X_test)\n",
    "# transfrom predictions to binary\n",
    "prediction_binary = (predictions > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy score\n",
    "accuracy = accuracy_score(y_true = Y_test, y_pred = prediction_binary)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the coefficients and their covariance matrix from the logistic regression fit\n",
    "beta_mean = model.params\n",
    "beta_cov = model.cov_params()\n",
    "# number of simulations\n",
    "n_simulations = 10000\n",
    "\n",
    "# simulate coefficients\n",
    "simulated_betas = multivariate_normal.rvs(mean = beta_mean, cov = beta_cov, size = n_simulations)\n",
    "\n",
    "# derive odds from log-odds coefficients \n",
    "simulated_betas_odds = np.exp(simulated_betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of each coefficient to assess and interpret them\n",
    "for name, i in zip(X_test.columns, range(simulated_betas_odds.shape[1])):\n",
    "    plt.figure()\n",
    "    plt.hist(simulated_betas_odds[:, i], bins=30, color='skyblue')\n",
    "    plt.axvline(x = np.median(simulated_betas_odds, axis = 0)[i], color = 'orange', label = f'Median {np.median(simulated_betas_odds, axis = 0)[i]:.2f}')\n",
    "    plt.axvline(x = np.percentile(simulated_betas_odds, 5, axis = 0)[i], color = 'orange', label = f'5th Percentile {np.percentile(simulated_betas_odds, 5, axis = 0)[i]:.2f}', linestyle = '--')\n",
    "    plt.axvline(x = np.percentile(simulated_betas_odds, 95, axis = 0)[i], color = 'orange', label = f'95th Percentile {np.percentile(simulated_betas_odds, 95, axis = 0)[i]:.2f}', linestyle = '--')\n",
    "    plt.title(f'{name}')\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Coefficients\n",
    "\n",
    "- The constant represents the baseline odds of having heart disease. This means that for a person with a poor or fair self perceived health status, who has not had a checkup in the past two years, who does not exercise, who does not have skin cancer or any other cancer, who does not have depression, who is not diabetic, who does not have arthritis, who is female, who is of average age, height, weight, and BMI, and who does not have a smoking history, the odds of having heart disease are 0.86 (0.83, 0.88). This indicates that the average person that falls into this category has a smaller chance of having heart disease than the chance of not having heart disease.\n",
    "\n",
    "- For individuals who *do* exercise, the odds of having heart disease are 1.24 (1.21, 1.28) relative to those that do not. This means that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the amount of components needed to perform PCA with a specific percentage of variance kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = data.drop(columns='Heart_Disease')\n",
    "\n",
    "# Percentage of variance that we want to keep, should be between 0 and 0.99\n",
    "percentage = 0.95\n",
    "components = 0\n",
    "\n",
    "# simulate a do-while loop\n",
    "while True:\n",
    "\n",
    "    pca = PCA(n_components=components)\n",
    "    pca_data = pca.fit_transform(X)\n",
    "\n",
    "    # if this is smaller than 0 we have kept at least percentage amount\n",
    "    # of variance in the PCA.\n",
    "    if percentage - sum(pca.explained_variance_ratio_) < 0:\n",
    "        break\n",
    "\n",
    "    components += 1\n",
    "\n",
    "print(f'Number of components to use to keep {percentage * 100:.0f}% of the variance: {components}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The exact same procedure, but now in a plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of variance that we want to keep, should be between 0 and 0.99\n",
    "percentage = 0.95\n",
    "components = 0\n",
    "\n",
    "# simulate a do-while loop\n",
    "components = 0\n",
    "ys = []\n",
    "while True:\n",
    "\n",
    "    pca = PCA(n_components=components)\n",
    "    pca.fit_transform(X)\n",
    "\n",
    "    # if this is smaller than 0 we have kept at least percentage amount\n",
    "    # of variance in the PCA.\n",
    "    ys.append(sum(pca.explained_variance_ratio_))\n",
    "    if percentage - sum(pca.explained_variance_ratio_) < 0:\n",
    "        break\n",
    "\n",
    "    components += 1\n",
    "\n",
    "print(f'Number of components to use to keep {percentage * 100:.0f}% of the variance: {components}')\n",
    "plt.scatter(range(len(ys)), ys, marker='x', color='red')\n",
    "plt.plot(range(len(ys)), ys, linestyle='--')\n",
    "plt.xticks(range(len(ys)))\n",
    "plt.axhline(percentage, color='green', linestyle='dashdot', label='Desired kept variance')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Kept variance (%)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Cross-Validation (k-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# selecting the target variable\n",
    "y = XY_constant['Heart_Disease']\n",
    "\n",
    "# define K for cross validation\n",
    "K = 100\n",
    "\n",
    "# setup the k-fold cross-validation\n",
    "kf = KFold(n_splits = K, shuffle = True)\n",
    "\n",
    "# dictionary for the model names and features\n",
    "# TODO: ensure PCA features are DataFrame, and ensure Y variable has same name\n",
    "pca_df = pd.DataFrame(pca_data)\n",
    "models = {'Base model': XY_constant.drop(columns='Heart_Disease'), 'PCA model': pca_df, 'Constant model': XY_constant['constant']}\n",
    "\n",
    "# initialize a dictionary to store accuracy data and aic data\n",
    "acc_dict = {key: [] for key in models.keys()}\n",
    "aic_dict = {key: [] for key in models.keys()}\n",
    "\n",
    "for key, df in models.items():\n",
    "\n",
    "    for train_index, test_index in kf.split(XY_constant):\n",
    "        \n",
    "        # Split into train and test according to the folds \n",
    "        X_train, X_test = df.iloc[train_index], df.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # For each fold split, fit the model\n",
    "        model = sm.Logit(y_train, X_train).fit()\n",
    "                \n",
    "        # Predict probabilities\n",
    "        y_pred_prob = model.predict(X_test)\n",
    "\n",
    "        # convert probabilities to binary predictions\n",
    "        y_pred_binary = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "        acc_dict[key].append(accuracy_score(y_true = y_test, y_pred = y_pred_binary))\n",
    "        aic_dict[key].append(model.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean accuracy of the baseline model\n",
    "mean_acc = np.mean(acc_dict['Base model'])\n",
    "\n",
    "#calculate average aic of all models\n",
    "average_aic = {key: sum(aic)/len(aic) for key, aic in aic_dict.items()}\n",
    "\n",
    "# plot the histogram of the PCA accuracies and overlay the baseline model's mean\n",
    "plt.hist(acc_dict['PCA model'], bins=30, color='skyblue', label = 'PCA Accuracies')\n",
    "plt.hist(acc_dict['Base model'], bins=30, color='teal', alpha=0.4, label='Complete model accuracies')\n",
    "plt.hist(acc_dict['Constant model'], bins=30, color='firebrick', alpha=0.3, label='Constant model')\n",
    "plt.title('Distribution of k-fold computed accuracies')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#print average aic scores\n",
    "print('average aic:', average_aic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
